from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import List, Optional, Dict, Any
import logging
from datetime import datetime, timedelta

from ..models.vulnerability_models import (
    VulnerabilityTool, VulnerabilityScanJob, StartVulnerabilityScanRequest,
    VulnerabilitySearchRequest, VulnerabilityStats, VulnerabilityScanResult,
    VulnerabilityInfo, VulnerabilityType, VulnerabilitySeverity
)
from ..services.database import get_database
from ..services.vulnerability_service import vulnerability_service

router = APIRouter(tags=["vulnerability"])
logger = logging.getLogger(__name__)

@router.get("/vulnerability-tools/status")
async def get_vulnerability_tools_status():
    """Get installation status of all vulnerability scanning tools"""
    tools_status = {}
    
    for tool in VulnerabilityTool:
        try:
            is_installed = await vulnerability_service.check_tool_installation(tool)
            tools_status[tool.value] = is_installed
        except Exception as e:
            logger.error(f"Error checking {tool} status: {str(e)}")
            tools_status[tool.value] = False
    
    installed_count = sum(1 for installed in tools_status.values() if installed)
    
    return {
        "tools": tools_status,
        "total_tools": len(VulnerabilityTool),
        "installed_tools": installed_count,
        "installation_percentage": round((installed_count / len(VulnerabilityTool)) * 100, 2)
    }

@router.post("/vulnerability-tools/install")
async def install_vulnerability_tools(
    tools: Optional[List[VulnerabilityTool]] = None,
    background_tasks: BackgroundTasks = None
):
    """Install vulnerability scanning tools"""
    if tools is None:
        tools = list(VulnerabilityTool)
    
    installation_results = {}
    
    for tool in tools:
        try:
            success, message = await vulnerability_service.install_tool(tool)
            installation_results[tool.value] = {
                "success": success,
                "message": message
            }
        except Exception as e:
            logger.error(f"Error installing {tool}: {str(e)}")
            installation_results[tool.value] = {
                "success": False,
                "message": f"Installation error: {str(e)}"
            }
    
    successful_installs = sum(1 for result in installation_results.values() if result["success"])
    
    return {
        "installation_results": installation_results,
        "successful_installs": successful_installs,
        "total_tools": len(tools),
        "success_rate": round((successful_installs / len(tools)) * 100, 2)
    }

@router.post("/targets/{target_id}/scan-vulnerabilities", response_model=VulnerabilityScanJob)
async def start_vulnerability_scan(
    target_id: str,
    request: StartVulnerabilityScanRequest,
    background_tasks: BackgroundTasks
):
    """Start vulnerability scanning for a target"""
    try:
        db = get_database()
        
        # Check if target exists
        target = await db.targets.find_one({"id": target_id})
        if not target:
            raise HTTPException(status_code=404, detail="Target not found")
        
        # Get URLs to scan
        urls_to_scan = []
        
        if request.urls:
            # Use provided URLs
            urls_to_scan = request.urls
        else:
            # Get URLs from JavaScript discovery results
            javascript_job = await db.javascript_jobs.find_one(
                {"target_id": target_id, "status": {"$in": ["completed", "partial"]}},
                sort=[("completed_at", -1)]
            )
            
            if javascript_job and javascript_job.get("results"):
                # Extract URLs from endpoints
                for result in javascript_job["results"]:
                    for endpoint in result.get("endpoints", []):
                        if endpoint.get("url"):
                            urls_to_scan.append(endpoint["url"])
                
                # Also include live subdomain URLs
                for result in javascript_job["results"]:
                    subdomain = result.get("subdomain")
                    if subdomain:
                        # Add HTTPS and HTTP variants
                        urls_to_scan.extend([
                            f"https://{subdomain}",
                            f"http://{subdomain}"
                        ])
            
            # If no JavaScript results, get from liveness check
            if not urls_to_scan:
                liveness_job = await db.liveness_jobs.find_one(
                    {"target_id": target_id, "status": {"$in": ["completed", "partial"]}},
                    sort=[("completed_at", -1)]
                )
                
                if liveness_job and liveness_job.get("results"):
                    for result in liveness_job["results"]:
                        if result.get("is_alive", False):
                            subdomain = result.get("subdomain")
                            if subdomain:
                                urls_to_scan.extend([
                                    f"https://{subdomain}",
                                    f"http://{subdomain}"
                                ])
        
        if not urls_to_scan:
            raise HTTPException(
                status_code=400,
                detail="No URLs available for scanning. Please run JavaScript discovery or liveness check first."
            )
        
        # Remove duplicates and limit URLs
        urls_to_scan = list(set(urls_to_scan))[:100]  # Limit to 100 URLs for performance
        
        # Check if there's already a running vulnerability scan
        existing_job = await db.vulnerability_jobs.find_one({
            "target_id": target_id,
            "status": {"$in": ["pending", "running"]}
        })
        if existing_job:
            raise HTTPException(
                status_code=400,
                detail="Vulnerability scan already running for this target"
            )
        
        # Create vulnerability scan job
        job = VulnerabilityScanJob(
            target_id=target_id,
            domain=target["domain"],
            urls=urls_to_scan,
            tools=request.tools if request.tools else list(VulnerabilityTool),
            total_urls=len(urls_to_scan),
            deep_scan=request.deep_scan,
            max_threads=request.max_threads,
            timeout_per_url=request.timeout_per_url,
            include_low_severity=request.include_low_severity,
            notes=request.notes
        )
        
        # Insert job into database
        await db.vulnerability_jobs.insert_one(job.dict())
        
        # Start vulnerability scanning in background
        background_tasks.add_task(
            vulnerability_service.scan_vulnerabilities,
            urls_to_scan,
            target_id,
            target["domain"],
            job.tools,
            request.deep_scan,
            request.max_threads,
            request.timeout_per_url
        )
        
        return job
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting vulnerability scan: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/vulnerability-jobs/{job_id}", response_model=VulnerabilityScanJob)
async def get_vulnerability_job(job_id: str):
    """Get vulnerability scan job details"""
    try:
        db = get_database()
        
        job = await db.vulnerability_jobs.find_one({"id": job_id})
        if not job:
            raise HTTPException(status_code=404, detail="Vulnerability scan job not found")
        
        return VulnerabilityScanJob(**job)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting vulnerability job {job_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/targets/{target_id}/vulnerability-jobs", response_model=List[VulnerabilityScanJob])
async def get_target_vulnerability_jobs(
    target_id: str,
    limit: int = 10,
    offset: int = 0
):
    """Get vulnerability scan jobs for a target"""
    try:
        db = get_database()
        
        # Check if target exists
        target = await db.targets.find_one({"id": target_id})
        if not target:
            raise HTTPException(status_code=404, detail="Target not found")
        
        # Get vulnerability scan jobs
        cursor = db.vulnerability_jobs.find(
            {"target_id": target_id}
        ).sort("created_at", -1).skip(offset).limit(limit)
        
        jobs = []
        async for job in cursor:
            jobs.append(VulnerabilityScanJob(**job))
        
        return jobs
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting vulnerability jobs for target {target_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/targets/{target_id}/vulnerability-results", response_model=List[VulnerabilityScanResult])
async def get_target_vulnerability_results(
    target_id: str,
    limit: int = 100,
    offset: int = 0,
    severity: Optional[VulnerabilitySeverity] = None,
    vulnerability_type: Optional[VulnerabilityType] = None,
    min_confidence: float = 0.0
):
    """Get vulnerability scan results for a target"""
    try:
        db = get_database()
        
        # Check if target exists
        target = await db.targets.find_one({"id": target_id})
        if not target:
            raise HTTPException(status_code=404, detail="Target not found")
        
        # Get the latest completed vulnerability scan job
        job = await db.vulnerability_jobs.find_one(
            {"target_id": target_id, "status": {"$in": ["completed", "partial"]}},
            sort=[("completed_at", -1)]
        )
        
        if not job:
            return []
        
        # Extract and filter results
        results = job.get("results", [])
        filtered_results = []
        
        for result_data in results:
            try:
                result = VulnerabilityScanResult(**result_data)
                
                # Apply filters to vulnerabilities
                if severity or vulnerability_type or min_confidence > 0.0:
                    filtered_vulns = []
                    
                    for vuln in result.vulnerabilities:
                        if severity and vuln.severity != severity:
                            continue
                        if vulnerability_type and vuln.vulnerability_type != vulnerability_type:
                            continue
                        if vuln.confidence_score < min_confidence:
                            continue
                        
                        filtered_vulns.append(vuln)
                    
                    result.vulnerabilities = filtered_vulns
                    
                    # Recalculate counts
                    result.total_vulnerabilities = len(filtered_vulns)
                    severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
                    for vuln in filtered_vulns:
                        severity_counts[vuln.severity.value] += 1
                    
                    result.critical_count = severity_counts["critical"]
                    result.high_count = severity_counts["high"]
                    result.medium_count = severity_counts["medium"]
                    result.low_count = severity_counts["low"]
                    result.info_count = severity_counts["info"]
                
                filtered_results.append(result)
                
            except Exception as e:
                logger.warning(f"Error processing vulnerability result: {str(e)}")
                continue
        
        # Apply pagination
        return filtered_results[offset:offset + limit]
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting vulnerability results for target {target_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/targets/{target_id}/vulnerabilities", response_model=List[VulnerabilityInfo])
async def get_target_vulnerabilities(
    target_id: str,
    limit: int = 100,
    offset: int = 0,
    severity: Optional[VulnerabilitySeverity] = None,
    vulnerability_type: Optional[VulnerabilityType] = None,
    min_confidence: float = 0.0
):
    """Get all vulnerabilities found for a target"""
    try:
        db = get_database()
        
        # Check if target exists
        target = await db.targets.find_one({"id": target_id})
        if not target:
            raise HTTPException(status_code=404, detail="Target not found")
        
        # Get vulnerability scan results
        results = await get_target_vulnerability_results(
            target_id, limit=1000, offset=0, severity=severity,
            vulnerability_type=vulnerability_type, min_confidence=min_confidence
        )
        
        # Extract all vulnerabilities from results
        all_vulnerabilities = []
        for result in results:
            all_vulnerabilities.extend(result.vulnerabilities)
        
        # Sort by severity and confidence
        all_vulnerabilities.sort(
            key=lambda v: (
                {"critical": 0, "high": 1, "medium": 2, "low": 3, "info": 4}.get(v.severity.value, 4),
                -v.confidence_score
            )
        )
        
        # Apply pagination
        return all_vulnerabilities[offset:offset + limit]
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting vulnerabilities for target {target_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post("/vulnerability/search", response_model=List[VulnerabilityInfo])
async def search_vulnerabilities(request: VulnerabilitySearchRequest):
    """Search vulnerabilities across all targets"""
    try:
        db = get_database()
        
        # Build aggregation pipeline
        pipeline = []
        
        # Match vulnerability jobs
        match_stage = {"status": {"$in": ["completed", "partial"]}}
        pipeline.append({"$match": match_stage})
        
        # Unwind results and vulnerabilities
        pipeline.extend([
            {"$unwind": "$results"},
            {"$unwind": "$results.vulnerabilities"}
        ])
        
        # Build filters
        filters = []
        
        if request.vulnerability_type:
            filters.append({"results.vulnerabilities.vulnerability_type": request.vulnerability_type.value})
        
        if request.severity:
            filters.append({"results.vulnerabilities.severity": request.severity.value})
        
        if request.min_confidence > 0.0:
            filters.append({"results.vulnerabilities.confidence_score": {"$gte": request.min_confidence}})
        
        if request.tool:
            filters.append({"results.vulnerabilities.tool_used": request.tool.value})
        
        if request.url_pattern:
            filters.append({"results.vulnerabilities.url": {"$regex": request.url_pattern, "$options": "i"}})
        
        if request.parameter_pattern:
            filters.append({"results.vulnerabilities.parameter": {"$regex": request.parameter_pattern, "$options": "i"}})
        
        if filters:
            pipeline.append({"$match": {"$and": filters}})
        
        # Project vulnerability data
        pipeline.append({
            "$project": {
                "vulnerability": "$results.vulnerabilities",
                "target_id": 1,
                "domain": 1
            }
        })
        
        # Sort by severity and confidence
        pipeline.append({
            "$sort": {
                "vulnerability.severity": 1,
                "vulnerability.confidence_score": -1
            }
        })
        
        # Apply pagination
        pipeline.extend([
            {"$skip": request.offset},
            {"$limit": request.limit}
        ])
        
        # Execute aggregation
        cursor = db.vulnerability_jobs.aggregate(pipeline)
        
        vulnerabilities = []
        async for doc in cursor:
            vuln_data = doc["vulnerability"]
            vuln_data["target_id"] = doc["target_id"]
            vuln_data["domain"] = doc["domain"]
            
            try:
                vulnerability = VulnerabilityInfo(**vuln_data)
                vulnerabilities.append(vulnerability)
            except Exception as e:
                logger.warning(f"Error parsing vulnerability data: {str(e)}")
                continue
        
        return vulnerabilities
        
    except Exception as e:
        logger.error(f"Error searching vulnerabilities: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/vulnerability/stats", response_model=VulnerabilityStats)
async def get_vulnerability_stats():
    """Get overall vulnerability scanning statistics"""
    try:
        db = get_database()
        
        # Get all completed vulnerability jobs
        cursor = db.vulnerability_jobs.find({"status": {"$in": ["completed", "partial"]}})
        
        stats = VulnerabilityStats()
        vulnerability_types = {}
        tools_used = {}
        
        # Calculate cutoff for last 24 hours
        cutoff_24h = datetime.utcnow() - timedelta(hours=24)
        
        async for job in cursor:
            stats.total_scans += 1
            
            # Check if job is from last 24 hours
            if job.get("completed_at") and job["completed_at"] >= cutoff_24h:
                stats.scans_last_24h += 1
            
            # Process job results
            for result in job.get("results", []):
                for vuln in result.get("vulnerabilities", []):
                    stats.total_vulnerabilities += 1
                    
                    # Count by severity
                    severity = vuln.get("severity", "info")
                    if severity == "critical":
                        stats.critical_vulnerabilities += 1
                    elif severity == "high":
                        stats.high_vulnerabilities += 1
                    elif severity == "medium":
                        stats.medium_vulnerabilities += 1
                    elif severity == "low":
                        stats.low_vulnerabilities += 1
                    else:
                        stats.info_vulnerabilities += 1
                    
                    # Count by type
                    vuln_type = vuln.get("vulnerability_type", "other")
                    vulnerability_types[vuln_type] = vulnerability_types.get(vuln_type, 0) + 1
                    
                    # Count by tool
                    tool = vuln.get("tool_used", "unknown")
                    tools_used[tool] = tools_used.get(tool, 0) + 1
                    
                    # Check if vulnerability is from last 24 hours
                    discovered_at = vuln.get("discovered_at")
                    if discovered_at and isinstance(discovered_at, datetime) and discovered_at >= cutoff_24h:
                        stats.vulnerabilities_last_24h += 1
        
        stats.vulnerability_types = vulnerability_types
        stats.tools_used = tools_used
        
        # Get top vulnerable domains
        pipeline = [
            {"$match": {"status": {"$in": ["completed", "partial"]}}},
            {"$group": {
                "_id": "$domain",
                "total_vulnerabilities": {"$sum": "$total_vulnerabilities"},
                "critical_vulnerabilities": {"$sum": "$critical_vulnerabilities"},
                "high_vulnerabilities": {"$sum": "$high_vulnerabilities"}
            }},
            {"$sort": {"total_vulnerabilities": -1}},
            {"$limit": 10}
        ]
        
        top_domains_cursor = db.vulnerability_jobs.aggregate(pipeline)
        top_domains = []
        
        async for domain_data in top_domains_cursor:
            top_domains.append({
                "domain": domain_data["_id"],
                "total_vulnerabilities": domain_data["total_vulnerabilities"],
                "critical_vulnerabilities": domain_data["critical_vulnerabilities"],
                "high_vulnerabilities": domain_data["high_vulnerabilities"]
            })
        
        stats.top_vulnerable_domains = top_domains
        
        return stats
        
    except Exception as e:
        logger.error(f"Error getting vulnerability stats: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")